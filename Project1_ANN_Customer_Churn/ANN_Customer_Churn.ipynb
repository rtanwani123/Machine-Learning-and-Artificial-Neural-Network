{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qscka4RBS_4Q"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# STRATEGIC & TACTICAL SOLUTION: Predicting Customer Churn in E-Commerce Using ANN\n",
        "# ===============================================================\n",
        "# Project Context:\n",
        "# This project demonstrates a strategic AI solution for predicting customer churn\n",
        "# using an Artificial Neural Network (ANN). Churn prediction is critical for business\n",
        "# strategy as it allows companies to proactively retain valuable customers.\n",
        "#\n",
        "# Tactical Relevance:\n",
        "# - ANN can capture complex non-linear relationships between customer behavior,\n",
        "#   purchase history, and engagement metrics.\n",
        "# - Enables actionable insights by predicting the probability of churn.\n",
        "#\n",
        "# Dataset:\n",
        "# Kaggle: \"E-Commerce Customer Churn Analysis and Prediction\"\n",
        "# URL (for reference in read-only manner): Please refer to README.md file in Github\n",
        "# ===============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "# -----------------------------------\n",
        "# STEP 1: Load Dataset (Read-Only)\n",
        "# -----------------------------------\n",
        "file_path = '/content/ANN/E-Commerce-Dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "print(f\"E-Commerce Dataset loaded from {file_path} (read-only, Kaggle URL in README.md file in Github)\")\n",
        "print(df.head())\n",
        "\n",
        "# -----------------------------------\n",
        "# STEP 2: Data Inspection & Cleaning\n",
        "# -----------------------------------\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nColumn Names:\", df.columns.tolist())\n",
        "print(\"\\nData Types:\\n\", df.dtypes)\n",
        "df.info()\n",
        "display(df.describe().T)\n",
        "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
        "\n",
        "for col in ['Tenure','WarehouseToHome','HourSpendOnApp','OrderAmountHikeFromlastYear',\n",
        "            'CouponUsed','OrderCount','DaySinceLastOrder']:\n",
        "    df[col] = df[col].fillna(0)\n",
        "\n",
        "df = df.drop('CustomerID', axis=1)\n",
        "\n",
        "# -----------------------------------\n",
        "# STEP 3: Separate Numerical & Categorical Columns\n",
        "# -----------------------------------\n",
        "numerical_cols = df.select_dtypes(include=['int64','float64']).columns\n",
        "categorical_cols = df.select_dtypes(include=['object','category']).columns\n",
        "\n",
        "# Encode categorical columns & store mapping for unseen data\n",
        "category_maps = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    category_maps[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "\n",
        "# -----------------------------------\n",
        "# STEP 4: Exploratory Data Analysis (EDA)\n",
        "# -----------------------------------\n",
        "df['Churn'].value_counts().plot(kind='bar', color=['skyblue','salmon'])\n",
        "plt.title(\"Churn Class Distribution\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "important_numeric = ['Tenure','HourSpendOnApp','OrderCount','CashbackAmount']\n",
        "df[important_numeric].hist(bins=20, figsize=(10,6), color='lightgreen')\n",
        "plt.suptitle(\"Key Numeric Feature Distributions\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14,12))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------\n",
        "# STEP 5: Handle Class Imbalance\n",
        "# -----------------------------------\n",
        "majority_class = df[df['Churn']==0]\n",
        "minority_class = df[df['Churn']==1]\n",
        "\n",
        "minority_over = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=42)\n",
        "df = pd.concat([majority_class, minority_over]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nBalanced class distribution:\\n\", df['Churn'].value_counts())\n",
        "\n",
        "# -----------------------------------\n",
        "# STEP 6: Train/Test Split & Feature Scaling\n",
        "# -----------------------------------\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# -----------------------------------\n",
        "# STEP 7: Build ANN Model (Strategic & Tactical Explanation)\n",
        "# -----------------------------------\n",
        "# - Dense: Fully connected layer linking all neurons from previous layer\n",
        "# - ReLU: Activation function introducing non-linearity, helping the model learn complex patterns\n",
        "# - Dropout: Regularization technique; randomly disables neurons to prevent overfitting\n",
        "# - Sigmoid: Output layer activation for binary classification (0=No churn, 1=Churn)\n",
        "# - Adam optimizer: Adaptive learning rate optimizer for faster convergence\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer (number of features = input shape)\n",
        "model.add(Input(shape=(X_train.shape[1],)))\n",
        "\n",
        "# Hidden Layer 1\n",
        "model.add(Dense(128, activation='relu'))  # 128 neurons, ReLU activation\n",
        "model.add(Dropout(0.1))  # Dropout 10% for regularization\n",
        "\n",
        "# Hidden Layer 2\n",
        "model.add(Dense(64, activation='relu'))  # 64 neurons, ReLU activation\n",
        "model.add(Dropout(0.2))  # Dropout 20% for regularization\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(1, activation='sigmoid'))  # 1 neuron, sigmoid activation for binary output\n",
        "\n",
        "print(\"\\n================ MODEL SUMMARY =================\")\n",
        "model.summary()\n",
        "print(\"==============================================\\n\")\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# EarlyStopping: stops training if validation loss does not improve for 5 consecutive epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# -----------------------------------\n",
        "# STEP 8: Train ANN Model\n",
        "# -----------------------------------\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    batch_size=64,\n",
        "                    epochs=50,\n",
        "                    callbacks=[early_stop])\n",
        "\n",
        "# -----------------------------------\n",
        "# STEP 9: Model Evaluation\n",
        "# -----------------------------------\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='green', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange', linewidth=2)\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------\n",
        "# STEP 10: Predict on Unseen Data (Simple Display)\n",
        "# -----------------------------------\n",
        "# Purpose:\n",
        "# Demonstrates how the trained ANN predicts churn for unseen customers.\n",
        "# Churn_Probability: Likelihood of churn (0 to 1)\n",
        "# Predicted_Label: 1 if probability > 0.5, else 0\n",
        "\n",
        "unseen_data = pd.DataFrame([\n",
        "    [4, 'Mobile Phone', 2, 6, 'Debit Card', 'Female', 2, 3, 'Laptop & Accessory', 5,\n",
        "     'Single', 9, 1, 11, 1, 1, 5, 230],\n",
        "    [0, 'Mobile Phone', 1, 6, 'Credit Card', 'Male', 1, 4, 'Laptop & Accessory', 3,\n",
        "     'Divorced', 2, 0, 17, 1, 1, 0, 120]\n",
        "], columns=X.columns)\n",
        "\n",
        "# Encode categorical columns\n",
        "for col in categorical_cols:\n",
        "    unseen_data[col] = unseen_data[col].map(category_maps[col]).fillna(-1)\n",
        "\n",
        "# Scale numeric features\n",
        "unseen_scaled = scaler.transform(unseen_data)\n",
        "\n",
        "# Predict probabilities\n",
        "pred_probs = model.predict(unseen_scaled)\n",
        "pred_labels = (pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Display results\n",
        "results = unseen_data.copy()\n",
        "results['Churn_Probability'] = pred_probs\n",
        "results['Predicted_Label'] = pred_labels\n",
        "results.index = [f'Customer_{i+1}' for i in range(len(results))]\n",
        "\n",
        "print(\"\\n=== PREDICTION RESULTS FOR UNSEEN CUSTOMERS ===\\n\")\n",
        "display(results)\n",
        "\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"- Churn_Probability: Higher values indicate higher likelihood of customer churn.\")\n",
        "print(\"- Predicted_Label: 1 = Likely to churn, 0 = Not likely to churn.\")\n"
      ]
    }
  ]
}